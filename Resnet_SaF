import torch
import torch.nn as nn
import torch.nn.functional as F
import pix_layer_cuda


# -----------------------------------------------------------
#  PiXOperator corresponds to dynamic channel selection F(X; p)
# -----------------------------------------------------------
class PiXOperator(torch.autograd.Function):
    @staticmethod
    def forward(ctx, gamma, tau, X, p):
        assert p.dim() == 2

        X = X.to(torch.float32)
        p = p.to(torch.float32)

        ctx.save_for_backward(X, p)
        ctx.gamma = gamma   # reduction factor γ
        ctx.tau = tau       # threshold τ

        outputs = pix_layer_cuda.forward(gamma, tau, X, p)
        return outputs[0]   # Y (reduced feature map)

    @staticmethod
    def backward(ctx, grad_output):
        X, p = ctx.saved_tensors

        grads = pix_layer_cuda.backward(
            ctx.gamma,
            ctx.tau,
            X,
            p,
            grad_output
        )

        grad_X = grads[0]
        grad_p = grads[1].view_as(p)

        return None, None, grad_X, grad_p


# -----------------------------------------------------------
#  PiX module = Dynamic channel reduction using sampling p
# -----------------------------------------------------------
class PiX(nn.Module):
    def __init__(self, reduction_ratio=4, tau=0.5):
        super().__init__()
        self.gamma = int(reduction_ratio)  # γ
        self.tau = tau                     # τ

    def forward(self, X, p):
        p_flat = p.squeeze(-1).squeeze(-1)   # p ∈ R^{C/γ}
        Y = PiXOperator.apply(self.gamma, self.tau, X, p_flat)
        return Y


# -----------------------------------------------------------
#  PixBlock = Holistic encoding + Inter-channel + Dynamic select
# -----------------------------------------------------------
class PixBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, reduction_ratio=4, fusion_mode='min'): 
        super().__init__()
        assert in_channels % reduction_ratio == 0

        self.fusion_mode = fusion_mode
        self.gamma = reduction_ratio
        self.reduced_channels = in_channels // reduction_ratio  # C/γ

        # Step 1: Holistic context encoding → z
        self.gca = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(in_channels, self.reduced_channels, 1), 
            nn.Sigmoid()
        )

        # Step 2: Dynamic channel selector F(X; p)
        self.pix_min = PiX(reduction_ratio)
        self.pix_max = PiX(reduction_ratio)
        self.pix_avg = PiX(reduction_ratio)

        self.bn_after_pix = nn.BatchNorm2d(self.reduced_channels)

        # Step 3: Conv for downstream feature extraction
        self.conv = nn.Sequential(
            nn.Conv2d(self.reduced_channels, out_channels, (3,1),
                      (stride,1), (1,0)),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.2)
        )

        # Shortcut
        self.shortcut = nn.Sequential()
        if in_channels != out_channels or stride != 1:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, (1,1), (stride,1)),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, X):
        p = self.gca(X)   # p = φ(z)

        # Y_base = selection via avg fusion
        Y_base = self.pix_avg(X, p)

        # For min/max/avg combinations
        X_reduced = X[:, :self.reduced_channels, :, :]

        if self.fusion_mode == 'min':
            v = torch.min(Y_base, X_reduced)
        elif self.fusion_mode == 'max':
            v = torch.max(Y_base, X_reduced)
        elif self.fusion_mode == 'avg':
            v = 0.5 * (Y_base + X_reduced)
        elif self.fusion_mode == 'min+max':
            v = 0.5 * (torch.min(Y_base, X_reduced) + torch.max(Y_base, X_reduced))
        elif self.fusion_mode == 'min+avg':
            v = 0.5 * (torch.min(Y_base, X_reduced) + 0.5*(Y_base + X_reduced))
        elif self.fusion_mode == 'max+avg':
            v = 0.5 * (torch.max(Y_base, X_reduced) + 0.5*(Y_base + X_reduced))
        else:
            raise ValueError(f"Unsupported fusion_mode: {self.fusion_mode}")

        # v_k → BN → conv
        v = self.bn_after_pix(v)
        out = self.conv(v)

        return F.relu(out + self.shortcut(X))


# -----------------------------------------------------------
#  Full Network (ResNet with Select-and-Fuse)
# -----------------------------------------------------------
class ResNet_PiX(nn.Module):
    def __init__(self, input_shape, num_classes, fusion_mode='min'):
        super().__init__()
        self.fusion_mode = fusion_mode

        # Entry conv
        self.entry = nn.Sequential(
            nn.Conv2d(1, 64, (3,1), padding=(1,0)),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )

        # 4 stages
        self.layers = nn.Sequential(
            self._make_layer(64, 64, 2, 2),
            self._make_layer(64, 96, 2, 2),
            self._make_layer(96, 128, 2, 2),
            self._make_layer(128, 160, 2, 2)
        )

        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, input_shape[-1]))
        self.dropout = nn.Dropout(p=0.5)
        self.fc = nn.Linear(160 * input_shape[-1], num_classes)

    def _make_layer(self, in_ch, out_ch, stride, blocks):
        layers = [PixBlock(in_ch, out_ch, stride, fusion_mode=self.fusion_mode)]
        for _ in range(1, blocks):
            layers.append(PixBlock(out_ch, out_ch, 1, fusion_mode=self.fusion_mode))
        return nn.Sequential(*layers)

    def forward(self, X):
        X = self.entry(X)
        X = self.layers(X)
        X = self.adaptive_pool(X)
        X = self.dropout(X)
        return self.fc(X)
